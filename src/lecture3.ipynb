{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Lecture 3\n",
    "## Multi-Linear Regression\n",
    "\n",
    "Run the following cell with `Shift + Enter` to watch the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/f87efac0\" frameborder=\"0\" allow=\"fullscreen\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following we load again the life expectancy dataset and show a pairs\n",
    "plot of all columns except the first one that contains the country names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.csv(file.path(\"..\", \"data\", \"life_expectancy.csv\"))\n",
    "pairs(data[,2:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us run a multiple linear regression on a training set and look at the\n",
    "`summary`. In the first line we select all columns except the first one and\n",
    "remove all rows that contain a missing value in some column with the command\n",
    "`na.omit`. The dot `.` in the formula of the linear regression means that we\n",
    "regress Y on all other columns, i.e. all other columns in the data set are taken\n",
    "as input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data <- na.omit(data[,2:6])\n",
    "idx.train <- sample(nrow(data), 3/4*nrow(data))\n",
    "data.train <- data[idx.train,]\n",
    "data.test <- data[-idx.train,]\n",
    "lm.fit <- lm(LifeExpectancy ~ ., data.train)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To see how some of the numbers shown by the `summary` command are computed, we\n",
    "define our own `quality.of.fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "quality.of.fit <- function(fit, data) {\n",
    "    RSS <- sum((data$LifeExpectancy - predict(fit, data))^2)\n",
    "    TSS <- sum((data$LifeExpectancy - mean(data$LifeExpectancy))^2)\n",
    "    cat(c(\"Residual standard error: \",\n",
    "          sqrt(1/(nrow(data) - length(coefficients(fit))) * RSS),\n",
    "          \" R^2 = \", 1 - RSS/TSS, \"\\n\"))\n",
    "}\n",
    "quality.of.fit(lm.fit, data.train)\n",
    "quality.of.fit(lm.fit, data.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Compare the outcome obtained here with the values obtained with the `summary`\n",
    "function in the previous cell.\n",
    "\n",
    "WARNING: The following 3 cells may run very slowly on the EPFL servers.\n",
    "There shouldn't be any issue, if you run it locally on decent hardware,\n",
    "but be prepared to wait when you run these cells on the EPFL servers (or don't\n",
    "run them at all).\n",
    "\n",
    "The following code loads the histopathologic cancer detection dataset, and plots\n",
    "randomly some of the images (shown in the slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://lcnwww.epfl.ch/bio322/PCaml.rda\"))\n",
    "\n",
    "par(mfrow = c(4, 6), oma = c(0, 0, 0, 0),\n",
    "    mar = c(.1, .1, .1, .1), pty=\"s\")\n",
    "for (i in sample(1:40000, 24)) {\n",
    "    image(t(1 - matrix(PCaml_x[i,], 32, 32)), col = gray.colors(256),\n",
    "          axes = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We prepare the data for fitting with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hpc <- data.frame(PCaml_x)\n",
    "data.hpc$Y <- PCaml_y\n",
    "mean(data.hpc$Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next we define a training and a test set, run logistic regression on the\n",
    "training set (note again that we use the dot `.` in the formula to regress on\n",
    "all other columns in the data frame, i.e. on all pixel values) and we plot the\n",
    "ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hpc.train <- data.hpc[1:30000,]\n",
    "data.hpc.test <- data.hpc[30001:40000,]\n",
    "fit <- glm(Y ~ ., data.hpc.train, family = \"binomial\")\n",
    "p <- predict(fit, data.hpc.test, type = \"response\")\n",
    "library(ROCR)\n",
    "plot(performance(prediction(p, data.hpc.test$Y), \"tpr\", \"fpr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quiz questions for this week are summarized in a single quiz.\n",
    "After this section you should be ready to answer the first two questions of the\n",
    "[quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1096745).\n",
    "\n",
    "## Confidence and Prediction Interval Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/ad995701\" frameborder=\"0\" allow=\"fullscreen\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us define a data generator function, draw two training sets and run linear\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generator <- function(b, N, sigma = 0.3) {\n",
    "    x <- runif(N)\n",
    "    data.frame(X = x, Y = b[1] + b[2] * x + sigma * rnorm(N))\n",
    "}\n",
    "set.seed(1)\n",
    "b <- c(2, 1.4)\n",
    "N <- 50\n",
    "data1 <- data.generator(b, N)\n",
    "data2 <- data.generator(b, N)\n",
    "plot(data1, col = \"red\")\n",
    "points(data2, col = \"blue\")\n",
    "abline(b, lwd = 2)\n",
    "fit1 <- lm(Y ~ ., data1)\n",
    "fit2 <- lm(Y ~ ., data2)\n",
    "abline(fit1, col = \"red\", lwd = 2)\n",
    "abline(fit2, col = \"blue\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If we include in the `predict` function the argument `interval = \"confidence\"`\n",
    "it will return an array with 3 columns (you may want to look at it with\n",
    "`head(conf.interval)`). The first column contains the prediction, the second the\n",
    "lower bound and the third the upper bound of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grid <- seq(0, 1, length.out = 100)\n",
    "conf.interval <- predict(fit1, data.frame(X = grid), interval = \"confidence\")\n",
    "plot(data1, col = \"red\")\n",
    "abline(b, lwd = 2)\n",
    "lines(grid, conf.interval[,1], col = \"red\", lwd = 2)\n",
    "lines(grid, conf.interval[,2], col = \"darkgreen\", lwd = 2)\n",
    "lines(grid, conf.interval[,3], col = \"darkgreen\", lwd = 2)\n",
    "abline(fit2, col = \"blue\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If instead we use the argument `interval = \"prediction\"` we get the prediction\n",
    "interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pred.interval <- predict(fit1, data.frame(X = grid), interval = \"prediction\")\n",
    "plot(data1, col = \"red\")\n",
    "abline(b, lwd = 2)\n",
    "lines(grid, pred.interval[,1], col = \"red\", lwd = 2)\n",
    "lines(grid, pred.interval[,2], col = \"darkgreen\", lwd = 2)\n",
    "lines(grid, pred.interval[,3], col = \"darkgreen\", lwd = 2)\n",
    "points(data2, col = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Everything we did for the one dimensional case, generalizes of course to\n",
    "multiple predictors. In the following cell we define a dataset with 3 predictors\n",
    "and, for the moment, a dummy response `Y` which is initialized with zeros.\n",
    "Then we use the function `model.matrix` to get the model matrix $\\boldsymbol{X}$\n",
    "shown in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(671)\n",
    "N <- 10^4\n",
    "data <- data.frame(X1 = runif(N), X2 = runif(N), X3 = runif(N), Y = rep(0, N))\n",
    "m <- model.matrix(Y ~ ., data)\n",
    "head(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next we generate a random coefficient vector and use it to generate the actual\n",
    "response values $Y$. The operator `%*%` is used to perform matrix\n",
    "multiplication (i.e. matrix times vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta <- rnorm(4)\n",
    "data$Y <- m %*% beta + .2 * rnorm(N)\n",
    "head(data$Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we run multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "idxs <- sample(10^4, 10^3)\n",
    "data.train <- data[idxs,]\n",
    "lm.fit <- lm(Y ~ ., data.train)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can check that the standard errors shown in the summary above are computed\n",
    "with the formula shown in the slides. The `solve` function performs the matrix\n",
    "inversion and the `t` function computes the matrix transform. The estimated\n",
    "standard deviation of the irreducible error is obtained via the `summary`\n",
    "function, which computes the `sigma` in the same way as our `quality.of.fit`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train <- model.matrix(Y ~ ., data.train)\n",
    "sqrt(diag(solve(t(m.train) %*% m.train)))*summary(lm.fit)$sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If we want to see the 95% confidence interval of the parameters we can use the\n",
    "function `confint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following cell we check whether approximately 95% of the test data is\n",
    "indeed within the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test <- data[-idxs,]\n",
    "p <- predict(lm.fit, data.test, interval = \"prediction\")\n",
    "mean(data.test$Y < p[,3] & data.test$Y > p[,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In those last two cells of this section we see that the width of the prediction\n",
    "interval is indeed computed with the formula given in the slides. We compute the\n",
    "prediction interval for the 20 first points in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p <- predict(lm.fit, data.test[1:20,], interval = \"prediction\")\n",
    "p[,3] - p[,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note that the model matrix `m.test` contains as rows the test points in the form\n",
    "we wanted it, e.g. we could see the first row of this matrix as the $x_0^T$ from\n",
    "the slides. The factor `qt((1 - .95)/2, lm.fit$df.residual)` used to scale the\n",
    "standard deviation is approximately equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.test <- model.matrix(Y ~ ., data.test[1:20,])\n",
    "qt((1-.95)/2, lm.fit$df.residual)*sqrt(diag(m.test %*% solve(t(m.train) %*% m.train) %*%\n",
    "t(m.test)) + 1)*summary(lm.fit)$sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Confounders, Heteroscedacity and the Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/4f57392d\" frameborder=\"0\" allow=\"fullscreen\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Confounders\n",
    "In this example we construct data with correlation between predictors `X1` and\n",
    "`X2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(19)\n",
    "N <- 100\n",
    "z <- runif(N)\n",
    "data <- data.frame(X1 = 0.7*z, X2 = z + .01*rnorm(N),\n",
    "                   X3 = runif(N), Y = rep(0, N))\n",
    "beta <- c(-.8, -.4, 2, -.5)\n",
    "m <- model.matrix(Y ~ ., data)\n",
    "data$Y <- m %*% beta + .1 * rnorm(N)\n",
    "pairs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can see that the standard errors for $\\beta_1$ and $\\beta_2$ are rather\n",
    "large, because an increase in $\\beta_1$ can be compensated with a decrease in\n",
    "$\\beta_2$ and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit <- lm(Y ~ ., data)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If we exclude `X2` from the fit, which can be done by writing `- X2` in the\n",
    "formula, the standard error for $\\beta_1$ is narrow. The reason is that we no\n",
    "longer have the possibility to compensate for a change in $\\beta_1$ with\n",
    "$\\beta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit <- lm(Y ~ . - X2, data)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The follwing code was used to generate the artificial shark attack dataset shown\n",
    "in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "icecream <- function(x) {.5*x}\n",
    "shark_attack <- function(x) {.03*x - 10}\n",
    "N <- 200\n",
    "swimmers <- floor(runif(N, 5, 500))\n",
    "icecream_consumed <- pmax(0, floor(icecream(swimmers) + 20*rnorm(N)))\n",
    "shark_attacks <- pmax(0, floor(shark_attack(swimmers) + 5*rnorm(N)))\n",
    "beach_data <- data.frame(swimmers, icecream_consumed, shark_attacks)\n",
    "pairs(beach_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = lm(shark_attacks ~ icecream_consumed)\n",
    "summary(fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 = lm(shark_attacks ~ swimmers + icecream_consumed)\n",
    "summary(fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following example both the predictor `X1` and the response `Y` are caused\n",
    "by an unobserved variable `Z`, but `Y` does not directly depend on `X1`.\n",
    "Of course, in the fit we see that `X1` is related to `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "set.seed(12)\n",
    "N <- 100\n",
    "z <- runif(N)\n",
    "data <- data.frame(X1 = 0.5*z, X2 = runif(N), X3 = runif(N))\n",
    "data$Y <- z + .1 * rnorm(N)\n",
    "summary(lm(Y ~ ., data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Heteroscedacity\n",
    "Let us now generate data with noise that depends on the predictor, i.e.\n",
    "heteroscedastic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generator1 <- function(b, N) {\n",
    "    x <- runif(N)\n",
    "    data.frame(X = x, Y = b[1] + b[2] * x + .1 * x^2 * rnorm(N))\n",
    "}\n",
    "set.seed(7)\n",
    "b <- c(.1, .5)\n",
    "N <- 50\n",
    "data <- data.generator1(b, N)\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit <- lm(Y ~ X, data)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We see that the prediction interval is too large for small `X` values and too\n",
    "small for large `X` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid <- seq(0, 1, length = 100)\n",
    "pred.interval <- predict(lm.fit, data.frame(X = grid), interval = \"prediction\")\n",
    "plot(data)\n",
    "abline(b)\n",
    "lines(grid, pred.interval[, 1], col = 'red', lwd = 2)\n",
    "lines(grid, pred.interval[, 2], col = 'darkgreen')\n",
    "lines(grid, pred.interval[, 3], col = 'darkgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We prepare now to estimate the standard errors of the coefficients by sampling\n",
    "many training sets. Our function `fitted.coefficients` returns the coefficients\n",
    "of a linear regression on some `data`. The `summarize` function takes many\n",
    "estimated coefficients as input and returns the mean and standard error per\n",
    "coefficient. We use again the `replicate` function to obtain `10^3` estimated\n",
    "coefficients with different training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.coefficients <- function(data) coef(lm(Y ~ ., data))\n",
    "summarize <- function(coeffs) {\n",
    "    m <- rowMeans(coeffs)\n",
    "    B <- ncol(coeffs)\n",
    "    data.frame(Estimate = m, Std.Error = sqrt(1/(B-1)*rowSums((coeffs - m)^2)))\n",
    "}\n",
    "B <- 10^3\n",
    "summarize(replicate(B, fitted.coefficients(data.generator1(b, N))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The `bootstrap` function takes a dataset as input and returns a new dataset of\n",
    "the same size, with rows sampled with replacement from the input dataset. You\n",
    "can see which rows are duplicated with the row numbers e.g. `43.2` indicates\n",
    "that this is the third time row `43` is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap <- function(data) data[sample(nrow(data), nrow(data), replace = T),]\n",
    "bootstrap(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now, without using multiple training sets, we estimate the standard errors of\n",
    "the coefficients with only a single training set and the bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(replicate(B, fitted.coefficients(bootstrap(data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In a similar way we can use the bootstrap to estimate confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.bootstrap <- summarize(replicate(B, predict(lm(Y ~ ., bootstrap(data)), data.frame(X = grid))))\n",
    "plot(data)\n",
    "abline(b)\n",
    "lines(grid, conf.bootstrap[,1], col = \"red\", lwd = 2, lty = 2)\n",
    "lines(grid, conf.bootstrap[,1] + 2*conf.bootstrap[,2], col = \"darkgreen\", lty = 2)\n",
    "lines(grid, conf.bootstrap[,1] - 2*conf.bootstrap[,2], col = \"darkgreen\", lty = 2)\n",
    "lines(grid, conf.interval[, 2], col = 'darkgreen')\n",
    "lines(grid, conf.interval[, 3], col = 'darkgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following cells there are more examples where the bootstrap leads to more\n",
    "accurate estimates of the standard errors of the coefficients than the linear\n",
    "regression formulas. In both cases the reason is that the true data generation\n",
    "process is non-linear and therefore the assumptions used to compute the standard\n",
    "errors with the formulas do not hold. The bootstrap does not rely on these\n",
    "assumptions and therefore leads to better estimates, usually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generator2 <- function(b, N) {\n",
    "    x <- runif(N)\n",
    "    data.frame(X = x, Y = b[1] + b[2] * x + b[3] * x^2 + .1 * rnorm(N))\n",
    "}\n",
    "set.seed(1)\n",
    "b <- c(0, -1, 5)\n",
    "data <- data.generator2(b, N)\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit <- lm(Y ~ X, data)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data)\n",
    "grid <- seq(0, 1, length.out = 100)\n",
    "p <- predict(lm.fit, data.frame(X = grid), interval = \"prediction\")\n",
    "plot(data)\n",
    "lines(grid, p[, 1], lwd = 2)\n",
    "lines(grid, p[, 2], col = 'blue')\n",
    "lines(grid, p[, 3], col = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(replicate(B, fitted.coefficients(data.generator2(b, N))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(replicate(B, fitted.coefficients(bootstrap(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generator3 <- function(b, N) {\n",
    "    data <- data.frame(X1 = rnorm(N), X2 = rnorm(N), X3 = rnorm(N), Y = rep(0, N))\n",
    "    m <- model.matrix(Y ~ . + X1*X2, data)\n",
    "    data$Y <- m %*% b + .01*rnorm(N)\n",
    "    data\n",
    "}\n",
    "N <- 100\n",
    "set.seed(11)\n",
    "b <- rnorm(5)\n",
    "data <- data.generator3(b, N)\n",
    "pairs(data)\n",
    "lm.fit <- lm(Y ~ ., data)\n",
    "summary(lm.fit)$coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summarize(replicate(B, fitted.coefficients(data.generator3(b, N))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "summarize(replicate(B, fitted.coefficients(bootstrap(data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be ready to answer questions 3 to 7 of the\n",
    "[quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1096745).\n",
    "\n",
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/e0a1c22a\" frameborder=\"0\" allow=\"fullscreen\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following cells we define the loss functions as the negative\n",
    "log-likelihood function for logistic and linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic <- function(x) 1/(1 + exp(-x))\n",
    "set.seed(12)\n",
    "b <- c(.5, -1)\n",
    "N <- 50\n",
    "x <- runif(N)\n",
    "data <- data.frame(X = x, Y = logistic(b[1] + b[2]*x) > runif(N))\n",
    "logistic.loss <- function(b) {\n",
    "    x <- data$X\n",
    "    y <- data$Y\n",
    "    p <- logistic(b[1] + b[2] * x)\n",
    "    -sum(y * log(p)) - sum((1 - y) * log(1 - p))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot.loss <- function(loss) {\n",
    "    grid1 <- seq(-3, 3, length = 20)\n",
    "    grid2 <- seq(-8, 8, length = 20)\n",
    "    z <- matrix(apply(expand.grid(grid1, grid2), 1, loss), 20, 20)\n",
    "    par(mfrow = c(1, 2))\n",
    "    persp(grid1, grid2, z, phi = 0, theta = 20, xlab = \"beta_0\", ylab = \"beta_1\", zlab = \"loss\")\n",
    "    contour(grid1, grid2, z, xlab = \"beta_0\", ylab = \"beta_1\")\n",
    "}\n",
    "plot.loss(logistic.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The negative log-likelihood function for linear regression under the Gaussian\n",
    "noise assumption would contain some constants that we omit here. Without these\n",
    "constants the negative log-likelihood function coincides with the sum of squared\n",
    "errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- data.frame(X = x, Y = b[1] + b[2] * x + .2 * rnorm(N))\n",
    "regression.loss <- function(b) {\n",
    "    x <- data$X\n",
    "    y <- data$Y\n",
    "    p <- b[1] + b[2] * x\n",
    "    sum((y - p)^2)\n",
    "}\n",
    "plot.loss(regression.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please head over to the final questions of the\n",
    "[quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1096745).\n",
    "\n",
    "## Conceptual Exercises\n",
    "\n",
    "No programming is required.\n",
    "\n",
    "**Q1.** I collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\varepsilon$ .\n",
    "\n",
    "(a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = \\beta_0 + \\beta_1 X + \\varepsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "(b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "(d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "**Q2.** How high is the probability that a given observation is in a given\n",
    "bootstrap sample? Maybe you want to make a guess, before you solve this\n",
    "exercise.\n",
    "\n",
    "We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of $n$ observations.\n",
    "\n",
    "(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What is the probability that the second bootstrap observation is not the jth observation from the original sample ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Argue that the probability that the jth observation is not in the bootstrap sample is $(1 - 1/n)^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) When $n = 5$, what is the probability that the jth observation is in the bootstrap sample ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) When $n = 100$, what is the probability that the jth observation is in the bootstrap sample ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f)  When $n = 10000$, what is the probability that the jth observation is in\n",
    "the bootstrap sample?\n",
    "\n",
    "(g) What is the probability in the limit $n \\to \\infty$?\n",
    "\n",
    "**Q3.** Let us assume the following simple data set with two-dimensional\n",
    "predictors and categorical responses with 3 categories: $x_1 = (1, 0)$, $y_1 = 1$,\n",
    "$x_2 = (1, 1)$, $y_2 = 2$, $x_3 = (0, 1)$, $y_3 = 3$.\n",
    "\n",
    "(a) Compute the loss of multinomial logistic regression if all coefficient\n",
    "vectors are equal to zero.\n",
    "\n",
    "(b) Compute the loss of multinomial logistic regression if $\\beta_i = x_i$ for\n",
    "$i = 1, 2, 3$ (we assume implicitly negligible intercepts $\\beta_{i0} = 0$).\n",
    "\n",
    "(c) Compute the posterior probabilities $\\mathrm{Pr}(Y = k| X = (0.5, 0))$ for\n",
    "the coefficient vectors in (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Exercises\n",
    "\n",
    "**Q4.** In this exercises we will look at confounders, non-linearity and\n",
    "heteroscedasticity in the life expectancy dataset.\n",
    "\n",
    "(a) Perform 4 times linear regression by fitting the life expectancy onto year,\n",
    "GDP, BMI and alcohol consumption separately.\n",
    "\n",
    "(b) Look at the summary of one of your fits and explain in your own words what\n",
    "the values in columns `Estimate` and `Std. Error` of the `Coefficients` table mean.\n",
    "\n",
    "(c) Are the slopes ($\\beta_1$) of the separate linear fits smaller, the same or larger\n",
    "than the corresponding estimated coefficients in the multiple linear regression?\n",
    "If they differ, explain what could be the reasons for the difference.\n",
    "\n",
    "(d) Perform a bootstrap estimate of the standard error of the coefficients.\n",
    "\n",
    "(e) Use the function `predict` to compute the prediction interval for the\n",
    "response given input Year = 2020, GDP = 80000, BMI = 25, Alcohol = 5. Explain in\n",
    "your own words what the resulting values mean.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
