{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10\n",
    "\n",
    "## Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/8cf0cf16\" frameborder=\"0\" allow=\"fullscreen\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The code in the following cell generates the figure you saw on the first slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "set.seed(123)\n",
    "data <- data.frame(X = 2*runif(30) - 1)\n",
    "data$Y <- sin(4*data$X) + .2*rnorm(30)\n",
    "\n",
    "# load libraries\n",
    "library(tree)\n",
    "library(keras)\n",
    "library(splines)\n",
    "\n",
    "# fit\n",
    "f1 <- tree(Y ~ X, data, subset = 1:20, minsize = 2)\n",
    "f2 <- keras_model_sequential() %>%\n",
    "        layer_dense(50, activation = \"relu\", input_shape = c(1)) %>%\n",
    "        layer_dense(50, activation = \"relu\", input_shape = c(1)) %>%\n",
    "        layer_dense(1) %>%\n",
    "        compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "fit(f2, data$X[10:30], data$Y[10:30], epochs = 2*10^3, verbose = 0)\n",
    "f3 <- smooth.spline(data$X[5:25], data$Y[5:25], df = 15)\n",
    "\n",
    "# predict\n",
    "grid <- seq(-1, 1, length = 100)\n",
    "yhat1 <- predict(f1, data.frame(X = grid))\n",
    "yhat2 <- predict(f2, grid)\n",
    "yhat3 <- predict(f3, grid)$y\n",
    "yhat.ensemble <- 1/3 * (yhat1 + yhat2 + yhat3)\n",
    "\n",
    "# plot\n",
    "plot(data)\n",
    "curve(sin(4*x), from = -1, to = 1, add = T, lwd = 3)\n",
    "lines(grid, yhat1, col = \"blue\")\n",
    "lines(grid, yhat2, col = \"red\")\n",
    "lines(grid, yhat3, col = \"darkgreen\")\n",
    "lines(grid, yhat.ensemble, col = \"orange\", lwd = 3)\n",
    "legend(-.6, 1.1, c(\"tree\", \"neural net\", \"spline\", \"ensemble\",\n",
    "                    \"true func.\"), lty = 1, col = c(\"blue\", \"red\",\n",
    "                    \"darkgreen\", \"orange\", \"black\"))\n",
    "\n",
    "# compare predications\n",
    "sapply(list(yhat1, yhat2, yhat3, yhat.ensemble),\n",
    "       function(yhat) mean((yhat - sin(4*grid))^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To understand bagging and random forests a bit better we will apply it to the\n",
    "XOR-problem. The following cell creates the XOR-data, fits a single tree and\n",
    "plots the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "library(MASS)\n",
    "set.seed(123)\n",
    "x1 <- mvrnorm(30, c(1.5, 1), .2*diag(2))\n",
    "x2 <- mvrnorm(30, c(-1, -1), .2*diag(2))\n",
    "x3 <- mvrnorm(30, c(-1, .8), .2*diag(2))\n",
    "x4 <- mvrnorm(30, c(.9, -1), .2*diag(2))\n",
    "data <- data.frame(X = rbind(x1, x2, x3, x4), Y = c(rep(1, 60), rep(0, 60)))\n",
    "\n",
    "# fit tree\n",
    "library(tree)\n",
    "t <- tree(as.factor(Y) ~ ., data, minsize = 30)\n",
    "print(t)\n",
    "\n",
    "# visualize tree\n",
    "par(mfrow = c(1, 2))\n",
    "plot(t)\n",
    "text(t)\n",
    "plot(data$X.1, data$X.2, col = data$Y + 2)\n",
    "abline(v = 1.50088, col = 'blue')\n",
    "lines(c(-3, 1.50088), c(-.460662, -.460662), col = 'blue')\n",
    "lines(c(-0.0517341, -0.0517341), c(-3, -.460662), col = 'blue')\n",
    "lines(c(0.950088, 0.950088), c(3, -.460662), col = 'blue')\n",
    "lines(c(-0.545005, -0.545005), c(3, -.460662), col = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we use the library `randomForest`.\n",
    "Recall that bagging is simply a special case of a random forest with $m = p$.\n",
    "We set $m$ in the code with `mtry`, i.e. to use bagging we set `mtry = 2`,\n",
    "which is the number of predictors for the XOR data.\n",
    "We perform bagging with $B$ = `ntree = 10` trees of at most 4 leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "bag <- randomForest(as.factor(Y) ~ ., data, mtry = 2, ntree = 10, maxnodes = 4)\n",
    "for (tree in 1:10) print(getTree(bag, tree, labelVar = T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "As you can see in the output of the cell above, each one of the 10 trees of this\n",
    "bag performs its first split along the `X.1` direction, roughly at value 1.5,\n",
    "similarly as we have seen it for the single tree above. This indicates that all\n",
    "split candidates in the `X.2` direction led to a smaller decrease in the loss.\n",
    "\n",
    "Now let's see what happens with a random forest, where for each split only one\n",
    "of the two predictors is considered as split candidate (`mtry = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "rf <- randomForest(as.factor(Y) ~ ., data, mtry = 1, ntree = 10, maxnodes = 4)\n",
    "for (tree in 1:10) print(getTree(rf, tree, labelVar = T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above you can see that some of the trees have their first split in\n",
    "`X.1` direction, while others have it in `X.2` direction. In this simple\n",
    "setting, where at each split only one of the predictors is randomly picked as\n",
    "split candidate, the direction of the first split is entirely determined by this\n",
    "random selection; no matter what the maximal decrease of a split candidate in\n",
    "the other direction would have been. Note that within each tree the different\n",
    "splits can be on different predictors, because the split candidate directions\n",
    "are randomly selected per node, and not per entire tree.\n",
    "\n",
    "Next, we apply bagging and random forests to the Heart data.\n",
    "By default, `randomForest()` uses `p/3` variables when building a random forest\n",
    "of regression trees, and `âˆšp` variables when building a random forest of\n",
    "classification trees.  In the cell below we use the `randomForest()` function to\n",
    "perform both random forests and bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heart <-read.csv(\"http://faculty.marshall.usc.edu/gareth-james/ISL/Heart.csv\")[,-1]\n",
    "Heart <- na.omit(Heart)\n",
    "Heart$AHD <- as.factor(Heart$AHD)\n",
    "Heart$ChestPain <- as.factor(Heart$ChestPain)\n",
    "Heart$Thal <- as.factor(Heart$Thal)\n",
    "Heart$Sex <- as.factor(Heart$Sex)\n",
    "library(randomForest)\n",
    "set.seed(2)\n",
    "bag <- randomForest(AHD ~ ., Heart, mtry = 13)\n",
    "rforest <- randomForest(AHD ~ ., Heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us now split the data an run bagging and random forests only on the test\n",
    "set and compute the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "train <- sample(nrow(Heart), nrow(Heart)/2)\n",
    "bag <- randomForest(AHD ~ ., Heart, subset = train, mtry = 13)\n",
    "rf <- randomForest(AHD ~ ., Heart, subset = train)\n",
    "pred.bag <- data.frame(predict(bag, Heart[-train,], predict.all = T))\n",
    "pred.rf <- data.frame(predict(rf, Heart[-train,], predict.all = T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The prediction data frames `pred.bag` and `pred.rf` return the predictions using\n",
    "the majority vote in the first column (`aggregrate`) and the predictions of each\n",
    "of the 500 trees in the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We define the `error.rate` function to compute the test error based on 2 to 500\n",
    "trees of all the trees fitted. Instead we could also run the fits with the\n",
    "`randomForest` function multiple times with the argument `ntree = 2` up to\n",
    "`ntree = 500` and use the first column of the `predict` function above. Having\n",
    "fitted already 500 trees, we can instead simply use them and our function\n",
    "`error.rate` to compute the test error rate for different numbers of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.rate <- function(ntree, y, pred) {\n",
    "    pred <- pred[,seq(2, ntree + 1)] == \"Yes\"\n",
    "    maj.vote <- rowMeans(pred) > 0.5\n",
    "    y <- y == \"Yes\"\n",
    "    mean(maj.vote != y)\n",
    "}\n",
    "test.err.bag <- sapply(2:500, error.rate, Heart[-train, 'AHD'], pred.bag)\n",
    "test.err.rf <- sapply(2:500, error.rate, Heart[-train, 'AHD'], pred.rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us plot the out-of-bag (OOB) and the test error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot(bag$err.rate[,1], type = 'l', ylim = c(.15, .35), col = 'darkgreen', ylab = \"Error Rate\", xlab = \"Number of Trees\")\n",
    "lines(rf$err.rate[,1], col = 'orange')\n",
    "lines(2:500, test.err.bag, col = 'blue')\n",
    "lines(2:500, test.err.rf, col = 'red')\n",
    "legend(\"topright\", c(\"OOB: Bagging\",\"Test: Bagging\", \"OOB: Random Forest\",\n",
    "                     \"Test: Random Forest\"),\n",
    "       col = c('darkgreen', 'blue', 'orange', 'red'), lty = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We see that the random forest has a slightly lower error rate than bagging.\n",
    "\n",
    "Using the `importance()` function, we can view the importance of each\n",
    "variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "importance(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots can be produced with the `varImpPlot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "varImpPlot(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Gini index decreases most with splits in variable `Thal`.\n",
    "(A little bit of background: the variable `Thal` reports the outcome of a\n",
    "Thallium Stress Test, also known as Nuclear Heart Scan. The outcome of this test\n",
    "can be \"normal\", meaning that blood flows normally at rest and during exercise.\n",
    "The outcome \"fixed\" means that some area of the heart does not get\n",
    "enough blood at rest and under stress, whereas the outcome \"reversable\" means that\n",
    "some area gets enough blood at rest but not under stress.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are ready, answer the question on the first page of\n",
    "[this quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1114498).\n",
    "\n",
    "## Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/3b931392\" frameborder=\"0\" allow=\"fullscreen\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run boosting we use the library `xgboost`.\n",
    "Because `xgboost` does not accept data frames we will first convert the data into ordinary matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "library(Matrix)\n",
    "heart.train.x <- sparse.model.matrix(AHD ~ . -1, data = Heart[train,])\n",
    "heart.test.x = sparse.model.matrix(AHD ~ . -1, data = Heart[-train,])\n",
    "heart.train.y = Heart[train, \"AHD\"] == \"Yes\"\n",
    "heart.test.y = Heart[-train, \"AHD\"] == \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train with the standard binary classification loss `\"binary:logistic\"`.\n",
    "For other objective functions see the documentation `?xgboost`.\n",
    "The arguments of `xgboost` are related to the terminology in the slides as\n",
    "follows: number of trees `nround` = $B$,\n",
    "learning rate (or shrinkage parameter) `eta` = $\\lambda$,\n",
    "maximal tree size `max_depth` = $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.heart = xgboost(heart.train.x, label = heart.train.y,\n",
    "                      objective = \"binary:logistic\",\n",
    "                      eta = 0.001,\n",
    "                      max_depth = 4,\n",
    "                      nround = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.boost = predict(boost.heart, heart.test.x)\n",
    "table(yhat.boost > 0.5, heart.test.y)\n",
    "mean((yhat.boost > 0.5) != heart.test.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is comparable to the one with Random Forests.\n",
    "\n",
    "Also in xgboost we can look at the importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot.importance(xgb.importance(model = boost.heart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following you see the code of the comparison between the different\n",
    "methods. We use the `Boston` housing data set. The function `getdata` in the\n",
    "following cell returns reproducible training and test splits that we will use\n",
    "later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "getdata <- function (seed = 1) {\n",
    "    set.seed(seed)\n",
    "    train = sample(1:nrow(Boston), nrow(Boston)/2)\n",
    "    list(train = train,\n",
    "         boston.test = Boston[-train,\"medv\"],\n",
    "         boston.train.x = as.matrix(Boston[train, names(Boston) != \"medv\"]),\n",
    "         boston.test.x = as.matrix(Boston[-train, names(Boston) != \"medv\"])\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the next cell we fit bagging, random forests and linear regression on 50\n",
    "different splits of the data. Have a look at `?with`, if you wonder what it\n",
    "does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "fit.and.evaluate <- function(seed = 1, method = randomForest, ...) {\n",
    "    with(getdata(seed), {\n",
    "             rf <- method(medv ~ ., Boston[train,], ...)\n",
    "             pred <- predict(rf, Boston[-train,])\n",
    "             mean((pred - Boston[-train,\"medv\"])^2)\n",
    "    })\n",
    "}\n",
    "rf.res <- sapply(1:50, fit.and.evaluate)\n",
    "bag.res <- sapply(1:50, fit.and.evaluate, mtry = 13)\n",
    "lm.res <- sapply(1:50, fit.and.evaluate, method = lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next we do the same for boosting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "boost.func <- function(seed = 1) {\n",
    "    with(getdata(seed), {\n",
    "        boston.train.xgb <- xgb.DMatrix(boston.train.x,\n",
    "                                   label = Boston[train, \"medv\"])\n",
    "        boston.test.xgb <- xgb.DMatrix(boston.test.x,\n",
    "                                   label = Boston[-train, \"medv\"])\n",
    "        boost.boston <- xgb.train(data = boston.train.xgb,\n",
    "                                 nround = 2000,\n",
    "                                 max_depth = 4,\n",
    "                                 eta = 0.005,\n",
    "                                 verbose = 0,\n",
    "                                 watchlist = list(train = boston.train.xgb,\n",
    "                                                  test = boston.test.xgb),\n",
    "                                 objective = \"reg:squarederror\")\n",
    "        yhat.boston <- predict(boost.boston, boston.test.xgb)\n",
    "        mean((yhat.boston - boston.test)^2)\n",
    "    })\n",
    "}\n",
    "boost.res <- sapply(1:50, boost.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "... and for neural networks. Here we go additionally through the hassle of\n",
    "scaling the data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "get.scale <- function(scaled) {\n",
    "    if (\"scaled:center\" %in% names(attributes(scaled))) {\n",
    "        center <- attr(scaled, \"scaled:center\")\n",
    "    } else {\n",
    "        center <- rep(0, ncol(scaled))\n",
    "    }\n",
    "    if (\"scaled:scale\" %in% names(attributes(scaled))) {\n",
    "        list(center, attr(scaled, \"scaled:scale\"))\n",
    "    } else {\n",
    "        list(center, rep(1., length(center)))\n",
    "    }\n",
    "}\n",
    "boston.x.scale <- function(x, scaled) {\n",
    "    s <- get.scale(scaled)\n",
    "    centered <- sweep(x, 2, s[[1]])\n",
    "    sweep(centered, 2, s[[2]], FUN = \"/\")\n",
    "}\n",
    "boston.y.scale <- function(y, scaled) {\n",
    "    s <- get.scale(scaled)\n",
    "    (y - s[[1]])/s[[2]]\n",
    "}\n",
    "boston.y.unscale <- function(y, scaled) {\n",
    "    s <- get.scale(scaled)\n",
    "    y * s[[2]] + s[[1]]\n",
    "}\n",
    "keras.func <- function (seed = 1) {\n",
    "    with(getdata(seed), {\n",
    "        boston.train.x.prep <- scale(boston.train.x, center = T, scale = T)\n",
    "        boston.train.y.prep <- scale(Boston[train, \"medv\"], center = T, scale = T)\n",
    "        nn <- keras_model_sequential()\n",
    "        nn <- nn %>%\n",
    "            layer_dense(200, activation = \"relu\", input_shape = c(13)) %>%\n",
    "            layer_dropout(rate = .25) %>%\n",
    "            layer_dense(200, activation = \"relu\") %>%\n",
    "            layer_dropout(rate = .25) %>%\n",
    "            layer_dense(1, activation = \"linear\")\n",
    "        nn %>% compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "        history <- nn %>% fit(boston.train.x.prep,\n",
    "                             boston.train.y.prep,\n",
    "                             verbose = 0,\n",
    "                             batch_size = length(boston.train.y.prep),\n",
    "                             validation_data = list(boston.x.scale(boston.test.x, boston.train.x.prep),\n",
    "                                                    boston.y.scale(boston.test, boston.train.y.prep)),\n",
    "                             epochs = 500)\n",
    "        nn.pred <- predict(nn, boston.x.scale(boston.test.x, boston.train.x.prep))\n",
    "        mean((boston.y.unscale(nn.pred, boston.train.y.prep) - boston.test)^2)\n",
    "    })\n",
    "}\n",
    "keras.res <- sapply(1:50, keras.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "After all this, let us produce the summary plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res <- data.frame(boost = boost.res, keras = keras.res,\n",
    "                  rforest = rf.res, bag = bag.res, linreg = lm.res)\n",
    "boxplot(res, ylab = \"test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can answer the question on the second page of\n",
    "[this quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1114498).\n",
    "\n",
    "## Beyond Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRdisplay::display_html('<iframe width=\"640\" height=\"360\" src=\"https://tube.switch.ch/embed/0e17cfca\" frameborder=\"0\" allow=\"fullscreen\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no code for this section.\n",
    "\n",
    "## Exercises\n",
    "## Conceptual\n",
    "\n",
    "**Q1.**\n",
    "We want to classify red versus blue points.\n",
    "The numbers next to the data point indicate the index of the data point in the\n",
    "training set, e.g. the 5th point of the training set has $X_1 = 2, X_2 = 2, Y =\n",
    "\\mbox{\"blue\"}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "echo": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAAP9NTU1oaGh8\nfHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD////xw1/KAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAUmElEQVR4nO3d61raSgCG0TEcAiJE7v9mKwEtIijKRw661o+KPm0z\ne8gLySR0ly1ws9L3AOA3EBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQI6CCkAiPzg708H04Pm4AkIUGAkCBASBAgJAgQEgQICQKE\nBAFCgoBOQ3pazNqLwLP66V6bgF50GFIzObqhYnqXTUBPOgypLtXjun20WVWlvscmoCcdhlSV\n9dvjdanusQnoSYchvbtB9vO7ZYXEyIz5HemGO9jZaeqqVHXT9zDG62X+pqv9w27PkVab9lHo\nHOm1o0+b5KJNtZ++Td8DGatpO3+L9nGXy9/To1W7yaevg9/ZxKp8sZbOBfP21awu874HMlLL\nMm22zXx/oNXtdaS6vY5UzRa560hNNfvxeP64wzGxQ+MfmrYv4Zv9wdW47mx4eHg4/dGsOMa/\n2vPz89F31SEkh8bXej9/ry9E7TXRMYXUVnSS0vrzky2OtHvB0a6wOBzaLXob0biczt+7d/RR\nhXT06ytvSNd7Pvq1tdytNlTLnoYzOqfzNym7ZZqnfkP6wXWkh5OvO2tnyld7Pvm6e0v6v+rE\nVz7M36LMmu16OrSQvvy3jc6FVJdVYjR/wocdYbk7tGvmxVvSVT6+ELWXD2ajO7Q7F1Jlxelq\nH3aESXtY3JRJP+MZm48hvbwIVYtfcY60Lta+r3d6jG/5+3s+nGO21vsXolGF9GHVbumw5BtO\nV532b+eN5e8rfZy/3Tv6cv9i3kNIy6pMvtj/r72ONDu6fY+vvb8OUpfdfXa1CwhXO52/+Xb7\nNCmPu2+6DGk92621LoIf7JtY/L7F9Iqngoua/b2K+7OLDkNat5t9ybjZbmafH5NduwmH97dp\n7/7uexDjtZm/ZNT93d/tPZL1/oj8i6UifTAynX+w7/Wd0Af7+E06D+lxf0zno+b8Kp0e2s1f\nlwaauX/8hF+ly3+Oq3o7nvvqU61CYmQ6vY5Uv+bz1VKRkBiZMd3ZAIMlJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB3Ye0\nnJQyW911E9C1DkMq7R+cllZ9l01AT7oOqS51s91u6rK8xyagJ12HVJVm97gpk3tsAnrSdUil\nHH0T3wT0pOuQ5q8hVffYBPSk05Bmi+WqPL48bOrPVxuExMh0GtJe+7Bq7rEJ6EmX15HW6+Vy\nNmuXHOpPOxISY+POBggQEgQICQL6Csl1JH6V4YRUjiU2Ad1xaAcBQoIAIUFAlyE181Kmh4/0\nWWzgV+kwpKZq1xFm+79ESPwmHYbUfpivWVbT9i8REr9JhyFV+z+4qSYbIfHLdP5vNry8KU2n\nQuKX6TCkSXm95XsyFRK/S4chLcv88GhTpkLiV+ly+bt+q2f1xV1AQmJkOr0gu569PtrMhcRv\n4s4GCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIA\nIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQ\nICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJ\nAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAh\nQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQICQKEBAFCgoBOQ3pazMrOrH661yagFx2G1EzKf9O7bAJ6cktIzfwliNXhh1//RXWp\nHtfto82qKnV4VNCnG0Jqqv1x2v6HX/9FVVm/PV6XKjwq6NMNIdVl+VLTsmqP0q4I6d1v+fz3\nC4mRuSGkav9gU0023pH4424I6bWdZjq99hxptWkfJc+RnkT3Y4eFn76HMV7reSnz/T59Q0iT\n0rw+ml4T0nZ6tGo3aT77ndePqqnsBz+1FtJtVu30Ve2ufENIyzI/PNqU6VVPx1PdXkeqZovY\ndaSZ/eDH1oeFIn6oqtbbZrY/uLpl+bt+24lX4de1S3/Z8/Pz+x88ekH9joeHh6PvlmXR20jG\n6f38PbYJNfvT/ZsuyK7fXtE28w5Cait6l9K1b4XstHvB0a6w3C27crXT+ZsfrZ6N6l6756Nf\n96ZlI6SrPRz9ujMrq3mpPl314cjp/E3KdlGV+f5sPxbS/OxPL/8l37+O9HzydbtdlMerVjnY\neTj5up1dc7cWrz7MXyn7U/79N9//C1//yHE6T99dPPu4/5dj5/7Ih5Dak2UhXevMjvD4cohf\nO8C7zpn52y02zPdnmrdcR6re1t7m4de160Ka7BYehXStDzvCXlMmnQ9llM6EtDtH2uzn74aQ\nXs7z929KL29H4Ve1q86R5mV3w6yQrnZ6jH9gBq90On+Hidt/ueUcaVm1b0q7t6PNTSO8vIl3\nTlbtPj8Q5IPTVacD83el0/mbpULaXYwq8+vfjq7/2MVV15GE9G3vr4NU7a0pG5dlr/Z+/hbt\nIdFmf1Zz46rd4u1zFF/7xscuvjEqGf1Yvbug2NRl1fdARmqzu9Ht5d3hcffNTSFtpu07UvV4\n1Z/7xscuhNSFwyubC0k/tTi6fHDTOVJ5PUeafXoL6sE3PnYhpE40dVUmFr9/bjV9u6Dd4ard\nNz52IQ5GpsPrSN/42IWQGJkO72z4xscuhMTIdHmv3fUfuxASI9Pp3d9Xf+xCSIzMmD5GAYMl\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkC\nhAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECAkChAQBQoKA7kNaTkqZre66CehahyGV9g9OS6u+yyagJ12HVJe62W43dVneYxPQk65D\nqkqze9yUyT02AT3pOqRSjr6JbwJ60nVI89eQqntsAnrSaUizxXJVHl8eNvXnqw1CYmQ6DWmv\nfVg199gE9KTL60jr9XI5m7VLDvWnHQmJsXFnAwQICQKEBAF9heQ6Er/KcEIqxxKbgO44tIMA\nIUGAkCBASBAgJAgQEgT0cNPqFSvcQmJkOgxpKSR+rU7v/q6m994E9KPTc6T1F/94UGAT0Itu\nFxuWZX3vTUAfrNpBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAjoN6WkxKzuz+ulem4BedBhSMyn/Te+yCehJhyHV\npXpct482q6rU99gE9KTDkKqyfnu8LtU9NgE96TCkUi59E9sE9MQ7EgR0e4602rSPgudIS83d\nYDkpVd30PYrRaualzA/vDl0uf0+PVu0mnz59V29i/fkhIp+q26eiUtIPVe387Uvq9jpS3V5H\nqmaL0HWkdSWkn1uXebN7T5/3PZCRqnczV5dZ+8247mx4fn4+/nZZpkL6jvfzN9vPnSm82sPD\nw9F3Vdm9lx+mb0whtXvB8a7wcqJlL7jeh/nbM4VXait6l9LOYdlsVCEd/dpa2wu+48P8tZov\n7jLh1cPRr2/qsmy/9hXSD64jPZ98veLv4cjZ+dsdH686H8ooPZx83Xksr8vPwwmpHDv3R4R0\nm/MhbapZ90MZpXMhLWdVWbSPRnRoJ6TbnJ2/pnJgd6VzIb2Y74/tRhTS2WN8IV3v3PxNJ32M\nZJzOniO9nGO2qw2jCunMqpOQrvdx/jaT6aav0YzP+VW7wy44ppA+XEfaCul7TuZvZcHue85d\nR9qU9k19XCGd+Z1C+rGNjm7S3tnQzMZ3jnT2dwrpx+afrZHyterow96dfh7pyxXu72/CXvBz\nVz0VfKKuymR/PbbLkJb3CAkGoctDu/XV1yyExMh0eo60/vzjfIlNQC+6XWxYHn3a/E6bgD6M\nfdUOBkFIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIGGhIMDI/2Mvz4Yxi29cwvtv8qfEJ\n6TLju82fGp+QLjO+2/yp8QnpMuO7zZ8an5AuM77b/KnxCeky47vNnxqfkC4zvtv8qfEJ6TLj\nu82fGp+QLjO+2/yp8QnpMuO7zZ8an5AuM77b/KnxCeky47vNnxrf0P9jYRSEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE9BDS8v0266pUddP9MC56P74f/6vq\nd7KcnEzXwObvdHwDm79mXsp8ffyT0Px1/1+4fj+r03aeJ50P46L341sPbEeo29FU/5/4gc3f\n6fiGNn9VO5qjklLz1/l/4bp6N6tPpVrvfvbU9TguORnfusx6G8oZ6zJvdu+Z89cfDGz+Poxv\nYPNX70ZWH40pNn9dh7Qs03c7al1WL78+lkXH47jkdHzLwYysNduP7f8QBzZ/H8Y3sPmryu69\n8ugZjs1f1yGVevtuR52VzXZIr1un41uWZW9juez/EIc2f3vHIQ1w/kr19jA2f12HtN6+31HL\n6UtYz07HNyur+cvJaG/jOasp09eHQ5u/1tH4hjh/9VHcsfnr4QkYdEjbDyG1phd/dx+W7fFI\na4Dz9258w5u/x1KOuhbS/ZyM7/HlFbYe1AHKpvp/IDLA+TsZ39Dmbzmrjs6IhHQ/Z8bSDGd9\n+WUw1dHr+wDn7934Dj8a0vxtt/P/Xf+akKrh7QjnxjKk8U2Pd8oBzt/0TDRDGt+u67fVhtj8\n9R3SftVkM6RVp2GHtJlMN0ffDm7+TsZ3MJz5a52uegbmr++QFu156aoMaF3n5B1zd+FhODvq\n6uS8fWjzdzq+gc3f63De3jRj89d3SAO7Mr9zcsG4bk+WVxd/e6c2p+tfA5u/D+Mb2Py1dzY0\ns//nSKO9s2H7f0fdf50MbXn0/fia/c1ZQ3nBn5f/964Ncf4+jG9g83e4166druz89R5S0959\n2/0oLvs4vslgFm/Lh5CGNX/nxzec+Wtv9j4MJzt/wzoJhJESEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBDSyMwP/5vG6e7/hrqz9BQOgWdhbKr2/yS8LNX+23XxFA6B\nZ2FsnkrZ7P4fx/v/D/e6EtIgeBZGZ3dwNzsc2C3LVEiD4FkYn6osXg/sSr0V0iB4Fsbn5eDu\ncGC3XW+FNAyehRGav67Y7QhpEDwLI1S9HtntCGkQPAvjMy+zo7ckIQ2CZ2F0nl7ej95OkoQ0\nEJ6F0anK4//rsUIaCM/C2Lwc2G2P7hAS0jB4FkbmqZTm5cvm7eBOSIPgWRiZ/a12RzfbCWkQ\nPAvj8nrz9/+DOyENgmcBAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI+Ae0i9of\nouRu1gAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(c(1, 2, 3, 1, 2, 3, 1, 2, 3), c(1, 1, 1, 2, 2, 2, 3, 3, 3),\n",
    "     col = c(\"red\", \"blue\")[c(1, 1, 2, 1, 2, 2, 2, 1, 1)],\n",
    "     xlab = \"X1\", ylab = \"X2\")\n",
    "text(c(1, 2, 3, 1, 2, 3, 1, 2, 3), c(1, 1, 1, 2, 2, 2, 3, 3, 3),\n",
    "     seq(1, 9), pos = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "a) Compute the entropy loss of the whole data set without considering a split.\n",
    "Use the logarithm $\\log_2$ with base 2 to compute the entropy.\n",
    "\n",
    "b) Compute the Gini index for the same setting.\n",
    "\n",
    "c) Find the split that maximally reduces the entropy loss and compute the\n",
    "reduction in loss and the value of the leaf nodes.\n",
    "Make sure to weight the contributions to the reduction in loss of the different\n",
    "regions with the number of data points they contain.\n",
    "\n",
    "d) Consider bagging with bootstrap training sets $b_1 = \\{1, 2, 3, 4, 5, 6, 7,\n",
    "8, 8\\}$ and $b_2 = \\{1, 2, 3, 4, 5, 6, 6, 7, 8\\}$. Where are the first splits of\n",
    "the trees fitted to $b_1$ and $b_2$, respectively?\n",
    "\n",
    "e) Compute the out-of-bag test error estimate for the bagged trees in d)\n",
    "\n",
    "f) Consider a Random Forest with $m=1$ and the same bootstrap training sets as\n",
    "in d).  Are the first splits of the two trees in this Random Forest at the same\n",
    "place as in bagging? Justify your answer.\n",
    "\n",
    "**Q2.**\n",
    "We want to apply boosting to the following regression problem.\n",
    "This time, the numbers next to the data points indicate the value of the\n",
    "response, e.g. our training set contains a data point with $X_1 = 1, X_2 = 1, Y = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": false
   },
   "outputs": [],
   "source": [
    "plot(c(1, 2, 3, 4), c(1, 4, 3, 2), xlab = \"X1\", ylab = \"X2\")\n",
    "text(c(1, 2, 3, 4), c(1, 4, 3, 2), c(1, 3, 5, 7), pos = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose $d = 1$, $B = 2$ and $\\lambda = 0.5$. Follow the boosting algorithm\n",
    "outlined in the slides of the lecture (it differs in the initialization from\n",
    "algorithm 8.2 from the book).\n",
    "\n",
    "a) Compute $f_0(X_1,X_2)$\n",
    "\n",
    "b) Compute the residuals and determine the splits and leaf values of the first\n",
    "tree and write $f_1(X_1,X_2)$ explicitly.\n",
    "\n",
    "c) Compute the residuals and determine the splits and leaf values of the second\n",
    "tree and write $f_2(X_1,X_2)$ explicitly.\n",
    "\n",
    "## Applied\n",
    "\n",
    "**Q3.**\n",
    "We now use boosting to predict `Salary` in the `Hitters` data set.\n",
    "The `Hitters` data is in `library(ISRL)`.\n",
    "\n",
    "a) Remove the observations for whom the salary information is unknown, and then\n",
    "log-transform the salaries.\n",
    "\n",
    "b) Create a training set consisting of the first 200 observations, and a test\n",
    "set consisting of the remaining observations.\n",
    "\n",
    "c) Perform boosting on the training set with 1000 trees for a range of values of\n",
    "the learning rate $\\lambda$. Produce a plot with different learning rate\n",
    "values on the $x$-axis and the corresponding training set MSE on the $y$-axis.\n",
    "\n",
    "d) Produce a plot with different learning rate values on the $x$-axis and the\n",
    "corresponding test set MSE on the $y$-axis.\n",
    "\n",
    "e) Compare the test MSE of boosting to the test MSE that results from applying\n",
    "linear regression.\n",
    "\n",
    "f) Which variables appear to be the most important predictors in the boosted\n",
    "model?\n",
    "\n",
    "g) Now apply bagging to the training set. What is the test set MSE for this\n",
    "approach?\n",
    "\n",
    "**Q4.**\n",
    "Use boosting to classify images in the Histopathalogic Cancer Detection data set\n",
    "that we studied in the last exercise of sheet 7* - part 2. Try different\n",
    "parameter values for $B$, $\\lambda$ and $d$. To tell R that the 0s and 1s in\n",
    "`PCaml_y` should be treated as values of a categorical response, you may use\n",
    "`PCaml_y <- as.factor(PCaml_y)`. Compare your results to the ones obtained with\n",
    "logistic regression and convolutional networks. Which are the important factors (pixels)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
